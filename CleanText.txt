collaborative filtering with temporal dynamics yehuda koren yahoo! research, haifa, israel yehuda abstract customer preferences for products are drifting over time. product perception and popularity are constantly changing as new selection emerges. similarly, customer inclinations are evolving, leading them to ever redefine their taste. thus, modeling temporal dynamics should be a key when designing recommender systems or general customer preference models. however, this raises unique challenges. within the ecosystem intersecting multiple products and customers, many different characteristics are shifting simultaneously, while many of them influence each other and often those shifts are delicate and associated with a few data instances. this distinguishes the problem from concept drift explorations, where mostly a single concept is tracked. classical timewindow or instancedecay approaches cannot work, as they lose too much signal when discarding data instances. a more sensitive approach is required, which can make better distinctions between transient effects and long term patterns. the paradigm we offer is creating a model tracking the time changing behavior throughout the life span of the data. this allows us to exploit the relevant components of all data instances, while discarding only what is modeled as being irrelevant. accordingly, we revamp two leading collaborative filtering recommendation approaches. evaluation is made on a large movie rating dataset by netflix. results are encouraging and better than those previously reported on this dataset. categories and subject descriptors h.2.8 [ database management ]: database applicationsdata mining general terms algorithms keywords collaborative filtering, recommender systems, concept drift 1. introduction modeling time drifting data is a central problem in data mining. often, data is changing over time, and up to date modeling should be continuously updated to reflect its present nature. the analysis of such data needs to find the right balance between discounting temporary effects that have very low impact on future behavior, while capturing longerterm trends that reflect the inherent nature of the data. this led to many works on the problem, which is also widely known as concept drift ; see e.g. [15, 25]. modeling temporal changes in customer preferences brings unique challenges. one kind of concept drift in this setup is the emergence of new products or services that change the focus of customers. related to this are seasonal changes, or specific holidays, which lead to characteristic shopping patterns. all those changes influence the whole population, and are within the realm of traditional studies on concept drift. however, many of the changes in user behavior are driven by localized factors. for example, a change in the family structure can drastically change shopping patterns. likewise, individuals gradually change their taste in movies and music. all those changes cannot be captured by methods that seek a global concept drift. instead, for each customer we are looking at different types of concept drifts, each occurs at a distinct time frame and is driven towards a different direction. the need to model time changes at the level of each individual significantly reduces the amount of available data for detecting such changes. thus we should resort to more accurate techniques than those that suffice for modeling global changes. for example, it would no longer be adequate to abandon or simply underweight far in time user transactions. the signal that can be extracted from those past actions might be invaluable for understanding the customer herself or be indirectly useful to modeling other customers. yet, we need to distill long term patterns while discounting transient noise. this requires a more sensitive methodology for addressing drifting customer preferences. it would not be adequate to concentrate on identifying and modeling just what is relevant to the present or the near future. instead, we require an accurate modeling of each point in the past, which will allow us to distinguish between persistent signal that should be captured and noise that should be isolated from the longer term parts of the model. modeling user preferences is relevant to multiple applications ranging from spam filtering to marketbasket analysis. our main focus in the paper is on modeling user preferences for building a recommender system, but we believe that general lessons that we learn would apply to other applications as well. automated recommendations is a very active research field [12]. such systems analyze patterns of user interest in items or products to provide personalized recommendations of items that will suit a user's taste. we expect user preferences to change over time. this may stem from multiple factors, some are fundamental while others are more circumstantial. for example, in a movie recommender system, users may change their preferred genre or adopt a new viewpoint on an actor or director. in addition, they may alter the appearance of their feedback. e.g., in a system where users provide star ratings to products, a user that used to indicate a neutral preference by a "3 stars" input, may now indicate dissatisfaction by the same "3 stars" feedback. similarly, it is known that user feedback is influenced by anchoring, where current ratings should be taken as relative to other ratings given at the same short period. finally, in many instances systems cannot separate different household members accessing the same account, even though each member has a different taste and deserves a separate model. this creates a de facto multifaceted metauser associated with the account. a way to get some distinction between different persons is by assuming that timeadjacent accesses are being done by the same member sometimes on behalf of other members, which can be naturally captured by a temporal model that assumes a drifting nature of a customer. all these patterns and the likes should have made temporal modeling a predominant factor in building recommender systems. nonetheless, with very few exceptions to be mentioned in sec. 7, the recommenders literature does not address temporal changes in user behavior. perhaps, because user behavior is composed of many different concept drifts, all acting in a different timeframe and different directions, thus making common methodologies for dealing with concept drift and temporal data less successful at this setup. we are showing that capturing time drifting patterns in user behavior is essential to improving accuracy of recommenders. this also gives us hope that the insights from successful time modeling for recommenders will be useful in other data mining applications. our test bed is a large movie rating dataset released by netflix as the basis of a well publicized competition [4]. this dataset combines several merits for the task at hand. first, it is not a synthetic dataset, but contains usermovie ratings by real paying netflix subscribers. in addition, its relatively large size above 100 million datestamped ratings makes it a better proxy for real life large scale datasets, while putting a premium on computational efficiency. finally, unlike some other dominant datasets, time effects are natural and are not introduced artificially. two interesting if not surprising temporal effects that emerge within this dataset are shown in fig. 1. one effect is an abrupt shift of rating scale that happened in early 2004. at that time, the mean rating value jumped from around 3.4 stars to above 3.6 stars. another significant effect is that ratings given to movies tend to increase with the movie age. that is, older movies receive higher ratings than newer ones. in sec. 6 we will return to these phenomena and use our temporal modeling to she would some light on their origins. the major contribution of this work is presenting a methodology and specific techniques for modeling time drifting user preferences in the context of recommender systems. the proposed approaches are applied on the aforementioned extensively analyzed movie ratings dataset, enabling us to firmly compare our methods with those reported recently. we show that by incorporating temporal information we achieve best results reported so far, indicating the significance of uncovering temporal effects. the rest of the paper is organized as follows. in the next section we describe basic notions and notation. then, in sec. 3 our principles for addressing time changing user preferences are evolved. those principles are then materialized, in quite different ways, within two leading recommender techniques: factor modeling sec. 4 and itemitem neighborhhod modeling sec. 5. in sec. 6 we describe an exploratory study, followed by surveying related work in sec. 7. 2. preliminaries we are given ratings about m users henceforth, interchangeable with "customers" and n items henceforth, interchangeable with "products". we reserve special indexing letters for distinguishing users from items: for users you, v , and for items i, j . we use t for time or, date. a rating r t indicates the preference by user you of item i at day t , where high values mean stronger preferences. figure 1: two temporal effects emerging within the netflix movie rating dataset. top: the average movie rating made a sudden jump in early 2004 1500 days since the first rating in the dataset. bottom: ratings tend to increase with the movie age at the time of the rating. here, movie age is measured by the time span since its first rating event within the dataset. in both charts each point averages 100,000 rating instances. for example, values can be integers ranging from 1 star indicating no interest to 5 stars indicating a strong interest. user you rates item i at most once, otherwise we take only the freshest rating, so given you and i , the day of rating is unique. sometimes, when the day of rating is not relevant, we will use the short notation r . we distinguish predicted ratings from known ones, by using the notation ^ r t for the predicted value of r t . usually the vast majority of ratings are unknown. the you, i, t triples for which r t is known are stored in the set k you, i, t r t is known . we evaluated our algorithms on a movie rating dataset of more than 100 million datestamped ratings performed by about half million anonymous netflix customers on 17,770 movies between dec 31, 1999 and dec 31, 2005 [4]. we are not aware of any publicly available comparable dataset that is close to the scope and quality of this one. to maintain compatibility with results published by others, we adopted some common standards. we evaluated our methods on two comparable sets designed by netflix: a holdout set "probe set" and a test set "quiz set", each of which contains over 1.4 million ratings. reported results are on the test set, while experiments on the holdout set show the same findings. in our timemodeling context, it is important to note that the test instances of each user come later in time than his/her training instances. the quality of the results is measured by their root mean squared error rmse: r ^ r / testset , a measure that puts more emphasis on large errors compared with the alternative of mean absolute error. achievable rmse values on the test set lie in a quite compressed range, as reported by many participants in the related competition. nonetheless, there is evidence that small improvements in rmse terms can have a significant impact on the quality of the top few presented recommendations [8]. recommender systems are often based on collaborative filtering cf, which relies only on past user behaviore.g., their previous transactions or product ratingsand does not require the creation of explicit profiles. when enough ratings were gathered per item, as in the netflix movie rating dataset, cf becomes the preferred and more accurate technique. notably, cf techniques require no domain knowledge and avoid the need for extensive data collection. in addition, relying directly on user behavior allows uncovering complex and unexpected patterns that would be difficult or impossible to profile using known data attributes. as a consequence, cf attracted much of attention in the past decade, resulting in significant progress and being adopted by some successful commercial systems, including amazon [9], tivo and netflix. in order to establish recommendations, cf systems need to compare fundamentally different objects: items against users. there are two primary approaches to facilitate such a comparison, which constitute the two main disciplines of cf: the neighborhood approach and latent factor models . neighborhood methods are centered on computing the relationships between items or, alternatively, between users. an itemitem approach [9, 14] evaluates the preference of a user to an item based on ratings of similar items by the same user. in a sense, these methods transform users to the item space by viewing them as baskets of rated items. latent factor models, such as matrix factorization, comprise an alternative approach by transforming both items and users to the same latent factor space, thus making them directly comparable. the latent space tries to explain ratings by characterizing both products and users on factors automatically inferred from user feedback. for example, when the products are movies, factors might measure obvious dimensions such as comedy vs. drama, amount of action, or orientation to children; less well defined dimensions such as depth of character development or "quirkiness"; or completely uninterpretable dimensions. 3. tracking drifting customer preferences one of the frequently mentioned examples of concept drift is changing customer preferences over time, e.g.: "customer preferences change as new products and services become available" [7]. this aspect of drifting customer preferences highlights a common paradigm in the literature of having global drifting concepts influencing the data as a whole. however, in many applications, including our focus application of recommender systems, we also face a more complicated form of concept drift where interconnected preferences of many users are drifting in different ways at different time points. this requires the learning algorithm to keep track of multiple changing concepts. in addition the typically low amount of data instances associated with individual customers calls for more concise and efficient learning methods, which maximize the utilization of signal in the data. in a survey on the problem of concept drift, tsymbal [22] argues that three approaches can be distinguished in the literature. the instance selection approach discards instances that are less relevant to the current state of the system. a common variant is time window approaches were only recent instances are considered. a possible disadvantage of this simple model is that it is giving the same significance to all instances within the considered time window, while completely discarding all other instances. this might be reasonable when the time shift is abrupt, but less so when time shift is gradual. thus, a refinement is instance weighting were instances are weighted based on their estimated relevance. frequently, a time decay function is used, underweighting instances as they occur deeper into the past. the third approach is based on ensemble learning , which maintains a family of predictors that together produce the final outcome. those predictors are weighted by their perceived relevance to the present time point, e.g., predictors that were more successful on recent instances get higher weights. we performed extensive experiments with instance weighting schemes, trying different exponential time decay rates on both neighborhood and factor models. the consistent finding was that prediction quality improves as we moderate that time decay, reaching best quality when there is no decay at all. this is despite the fact that users do change their taste and rating scale over the years, as we show later. however, much of the old preferences still persist or, more importantly, help in establishing useful crossuser or crossproduct patterns in the data. thus, just underweighting past actions loses too much signal along with the lost noise, which is detrimental given the scarcity of data per user. as for ensemble learning, having multiple models, each of which considers only a fraction of the total behavior may miss those global patterns that can be identified only when considering the full scope of user behavior. what makes them even less appealing in our case is the need to keep track of the independent drifting behaviors of many customers. this, in turn, would require building a separate ensemble for each user. such a separation will significantly complicate our ability to integrate information across users along multiple time points, which is the cornerstone of collaborative filtering . for example, an interesting relation between products can be established by related actions of many users, each of them at a totally different point of time. capturing such a collective signal requires building a single model encompassing all users and items together. all those considerations led us to the following guidelines we adopt for modeling drifting user preferences. we seek models that explain user behavior along the full extent of the time period, not only the present behavior while subject to performance limitations. this is key to being able to extract signal from each time point, while neglecting only the noise. multiple changing concepts should be captured. some are userdependent and some are itemdependent. similarly, some are gradual while others are sudden. while we need to model separate drifting "concepts" or preferences per user and/or item, it is essential to combine all those concepts within a single framework. this allows modeling interactions crossing users and items thereby identifying higher level patterns. in general, we do not try to extrapolate future temporal dynamics, e.g., estimating future changes in a user's preferences. this could be very helpful but is seemingly too difficult, especially given a limited amount of known data. rather than that, our goal is to capture past temporal patterns in order to isolate persistent signal from transient noise. this, indeed, helps in predicting future behavior. now we turn to how these desirable principles materialize into concrete methods when dealing with two leading approaches to collaborative filtering matrix factorization and neighborhood models. 4. timeaware factor model 4.1 the anatomy of a factor model one of the more successful approaches to cf is based on a matrix factorization model [2, 5, 10, 13, 18]. this approach lends itself well to an adequate modeling of temporal effects. before we deal with those temporal effects, we would like to establish the foundations of a static factor model. matrix factorization models map both users and items to a joint latent factor space of dimensionality f , such that ratings are modeled as inner products in that space. accordingly, each user you is associated with a vector p r and each item i is associated with a vector q r . a rating is predicted by the rule: ^ r q p 1 in order to learn the vectors p and q we minimize the regularized squared error: min r q p l q p the constant l controls the extent of regularization, as usually determined by cross validation. minimization is typically performed by either stochastic gradient descent or alternating least squares. such a pure factor model serves well in capturing the interaction between users and items. however, much of the observed rating values are due to effects associated with either users or items, independently of their interaction. a prime example is that typical cf data exhibit large user and item biases i.e., systematic tendencies for some users to give higher ratings than others, and for some items to receive higher ratings than others. we will encapsulate those effects, which do not involve useritem interaction, within the baseline predictors . these baseline predictors tend to capture much of the observed signal, in particular much of the temporal dynamics within the data. hence, it is vital to model them accurately, which enables better identification of the part of the signal that truly represents useritem interaction and should be subject to factorization. a suitable way to construct a static baseline predictor is as follows. denote by you the overall average rating. a baseline predictor for an unknown rating r is denoted by b and accounts for the user and item main effects: b you b b 2 the parameters b and b indicate the observed deviations of user you and item i , respectively, from the average. for example, suppose that we want a baseline estimate for the rating of the movie titanic by user joe. now, say that the average rating over all movies, you , is 3.7 stars. furthermore, titanic is better than an average movie, so it tends to be rated 0.5 stars above the average. on the other hand, joe is a critical user, who tends to rate 0.3 stars lower than the average. thus, the baseline estimate for titanic's rating by joe would be 3.9 stars by calculating 3 . 7 0 . 3 0 . 5 . the baseline predictor should be integrated back into the factor model. to achieve this we extend rule 1 to be: ^ r you b b q p 3 here main effects are explicitly isolated, thus letting the q p factorization deal effectively with the relevant portions of the signal; see also [8, 10]. the factor model we are using in this work is svd [8], which slightly differs from 3. this model was shown to offer a superior accuracy by also accounting for the more implicit information recorded by which items were rated regardless of their rating value. to this end a second set of item factors is added, relating item i to a factor vector y r . those new item factors are used to characterize users based on the set of items that they rated. the exact model is as follows see [8] for further explanations: ^ r you b b q p r you y 4 the set r you contains the items rated by user you . the decomposition of a rating into distinct portions is convenient here, as it allows us to treat different temporal aspects in separation. more specifically, we identify the following effects: 1 user biases b change over time; 2 item biases b change over time; 3 user preferences p change over time. on the other hand, we would not expect a significant temporal variation of item characteristics q , as items, unlike humans, are static in their nature. we start with a detailed discussion of the temporal effects that are contained within the baseline predictors. 4.2 time changing baseline predictors much of the temporal variability is included within the baseline predictors, through two major temporal effects. first is addressing the fact that an item's popularity is changing over time. for example, movies can go in and out of popularity as triggered by external events such as the appearance of an actor in a new movie. this is manifested in our models by the fact that item bias b will not be a constant but a function that changes over time. the second major temporal effect is related to user biases users change their baseline ratings over time. for example, a user who tended to rate an average movie "4 stars", may now rate such a movie "3 stars", for various reasons explained earlier. hence, in our models we would like to take the parameter b as a function of time. this induces the following template for a time sensitive baseline predictor: b t you b t b t 5 the function b t represents the baseline estimate for you 's rating of i at day t . here, b t and b t are real valued functions that change over time. the exact way to build these functions should reflect a reasonable way to parameterize the involving temporal changes. we will detail our choice in the context of the movie rating dataset, which demonstrates some typical considerations. a major distinction is between temporal effects that span extended periods of time and more transient effects. in the movie rating case, we do not expect movie likeability to fluctuate on a daily basis, but rather to change over more extended periods. on the other hand, we observe that user effects can change on a daily basis, reflecting inconsistencies natural to customer behavior. this requires finer time resolution when modeling userbiases compared to a lower resolution that suffices for capturing itemrelated time effects. let us start with our choice of timechanging item biases b t , which are easier to capture since we do not need finest resolution there. thus, an adequate decision would be to split the item biases into timebased bins. during each time period corresponding to a single bin we use a distinct item bias. the decision of how to split the timeline into bins should balance the desire to achieve finer resolution hence, smaller bins with the need to have enough ratings per bin hence, larger bins. for the movie rating data, there is a wide variety of bin sizes that yield about the same accuracy. in our implementation each bin corresponds to roughly ten consecutive weeks of data, leading to an overall number of 30 bins spanning all days in the dataset. a day t is associated with an integer bin t a number between 1 and 30 in our data, such that the movie bias is split into a stationary part and a time changing part: b t b b 6 while binning the parameters works well on the items, it is more of a challenge on the users side. on the one hand, we would like a finer resolution for users to detect very short lived temporal effects. on the other hand, we do not expect having enough ratings per user to produce reliable estimates for isolated bins. different function forms can be considered for parameterizing temporal user behavior, with varying complexity and accuracy. the first modeling choice is very concise, and uses a linear function to capture a possible gradual drift of user bias. let us first introduce some new notation. for each user you , we denote the mean date of rating by t . now, if you rated a movie on day t , then the associated time deviation of this rating is defined as: dev t sign t t t t here t t measures the time distance e.g., number of days between dates t and t . we set the value of b by cross validation; in our implementation b 0 . 4 . we introduce a single new parameter for each user called a so that we get our first definition of a timedependent userbias: b t b a dev t 7 this offers a simple linear model for approximating a drifting behavior, which requires learning two parameters per user: b and a . a more flexible parameterization is offered by splines. let you be a user associated with n ratings. we designate k time points t , . . . , t spaced uniformly across the dates of you 's ratings as kernels that control the following function: b t b e b e 8 the parameters b are associated with the control points or, kernels, and are automatically learnt from the data. this way the user bias is formed as a timeweighted combination of those parameters. the number of control points, k , balances flexibility and computational efficiency. in our application we set k n , letting it grow with the number of available ratings. the constant g determines the smoothness of the spline; we set g 0 . 3 by cross validation. so far we have discussed smooth functions for modeling the user bias, which mesh well with gradual concept drift . however, in many applications there are sudden drifts emerging as "spikes" associated with a single day or session. for example, in the movie rating dataset we have found that multiple ratings a user gives in a single day, tend to concentrate around a single value. such an effect does not span more than a single day. this may reflect the mood of the user that day, the impact of ratings given in a single day on each other, or changes in the actual rater in multiperson accounts. to address such short lived effects, we assign a single parameter per user and day, absorbing the dayspecific variability. this parameter is denoted by b . notice that in some applications the basic primitive time unit to work with can be shorter or longer than a day. e.g., our notion of day can be exchanged with a notion of a user session. in the netflix movie rating data, a user rates on 40 different days on average. thus, working with b requires, on average, 40 parameters to describe each user bias. it is expected that b is inadequate as a standalone for capturing the user bias, since it misses all sorts of signals that span more than a single day. thus, it serves as an additive component within the previously described schemes. the timelinear model 7 becomes: b t b a dev t b 9 similarly, the splinebased model becomes: b t b e b e b 10 model static mov linear spline linear spline rmse .9799 .9771 .9731 .9714 .9605 .9603 table 1: comparing baseline predictors capturing main movie and user effects. as temporal modeling becomes more accu rate, prediction accuracy improves lowering rmse. a baseline predictor on its own cannot yield personalized recommendations, as it misses all interactions between users and items. in a sense, it is capturing the portion of the data that is less relevant for establishing recommendations and in doing so enables deriving accurate recommendations. nonetheless, to better assess the relative merits of the various choices of timedependent userbias, we will compare their accuracy as standalone predictors. in order to learn the involved parameters we minimize the associated regularized squared error by using stochastic gradient descent. for example, in our actual implementation we adopt rule 9 for modeling the drifting user bias, thus arriving at the baseline predictor: b t you b a dev t b b b 11 to learn the involved parameters, b , a , b , b and b , one should solve: min r t you b a dev t b b b l b a b b b here, the first term strives to construct parameters that fit the given ratings. the regularization term, l b . . . , avoids overfitting by penalizing the magnitudes of the parameters, assuming a neutral 0 prior. learning is done by a stochastic gradient descent algorithm running 2030 iterations, with l 0 . 01 . table 1 compares the ability of various suggested baseline predictors to explain signal in the data. as usual, the amount of captured signal is measured by the root mean squared error on the test set. as a reminder, test cases come later in time than the training cases for the same user. we code the predictors as follows: static no temporal effects: b t you b b . mov accounting only to movierelated temporal effects: b t you b b b . linear linear modeling of user biases: b t you b a dev t b b . spline spline modeling of user biases: b t you b b b . linear linear modeling of user biases and single day effect: b t you b a dev t b b b . spline spline modeling of user biases and single day effect: b t you b b b b . the table shows that while temporal movie effects reside in the data lowering rmse from 0.9799 to 0.9771, the drift in user biases is much more influential. the additional flexibility of splines at modeling user effects leads to better accuracy compared to a linear model. however, sudden changes in user biases, which are captured by the perday parameters, are most significant. indeed, when including those changes, the difference between linear modeling "linear" and spline modeling "spline" virtually vanishes. beyond the temporal effects described so far, one can use the same methodology to capture more effects. a prime example is capturing periodic effects. for example, some products can be more popular in specific seasons or near certain holidays. similarly, different types of television or radio shows are popular throughout different segments of the day known as "dayparting". periodic effects can be found also on the user side. as an example, a user may have different attitudes or buying patterns during the weekend compared to the working week. a way to model such periodic effects is to dedicate a parameter for the combinations of time periods with items or users. this way, the item bias of 6, becomes: b t b b b e.g., if we try to capture the change of item bias with the season of the year, then period t fall , winter , spring , summer . similarly, recurring user effects are modeled by modifying 9 to be: b t b a dev t b b e.g., if we try to model a dayofweek user effect, then period t sun , mon , tue , wed , thu , fri , sat . we could not find periodic effects with a significant predictive power within the movierating dataset, thus our reported results do not include those. another temporal effect within the scope of basic predictors is related to the changing scale of user ratings. while b t is a userindependent measure for the merit of item i at time t , users tend to respond to such a measure differently. for example, different users employ different rating scales, and a single user can change his rating scale over time. accordingly, the raw value of the movie bias is not completely userindependent. to address this, we add a timedependent scaling feature to the baseline predictors, denoted by c t . thus, the baseline predictor 11 becomes: b t you b a dev t b b b c t 12 all discussed ways to implement b t would be valid for implementing c t as well. we chose to dedicate a separate parameter per day, resulting in: c t c c . as usual, c is the stable part of c t , whereas c represents dayspecific variability. adding the multiplicative factor c t to the baseline predictor lowers rmse to 0.9555. interestingly, this basic model, which captures just main effects disregarding useritem interactions, can explain almost as much of the data variability as the commercial netflix cinematch recommender system, whose published rmse on the same test set is 0.9514 [4]. 4.3 time changing factor model in the previous subsection we discussed the way time affects baseline predictors. however, as hinted earlier, temporal dynamics go beyond this, they also affect user preferences and thereby the interaction between users and items. users change their preferences over time. for example, a fan of the "psychological thrillers" genre may become a fan of "crime dramas" a year later. similarly, humans change their perception on certain actors and directors. this effect is modeled by taking the user factors the vector p as a function of time. once again, we need to model those changes at the very fine level of a daily basis, while facing the builtin scarcity of user ratings. in fact, these temporal effects are the hardest to capture, because preferences are not as pronounced as main effects userbiases, but are split over many factors. the same way we treat user biases we also treat each component of the user preferences p t p t , . . . , p t . in our application, we have found modeling after 9 effective leading to: p t p a dev t p k 1 , . . . , f 13 here p captures the stationary portion of the factor, a dev t approximates a possible portion that changes linearly over time, and p absorbs the very local, dayspecific variability. model f 10 f 20 f 50 f 100 f 200 svd .9140 .9074 .9046 .9025 .9009 svd .9131 .9032 .8952 .8924 .8911 timesvd .8971 .8891 .8824 .8805 .8799 table 2: comparison of three factor models: prediction accuracy is measured by rmse lower is better for varying factor dimensionality f . for all models accuracy improves with growing number of dimensions. most significant accuracy gains are achieved by addressing the temporal dynamics in the data through the timesvd model. at this point, we can tie all pieces together and extend the svd factor model by incorporating the time changing parameters. this leads to a model, which will be denoted as timesvd , where the prediction rule is as follows: ^ r t you b t b t q p t r you y 14 the exact definitions of the time drifting parameters b t , b t and p t were given in 6,9 and 13. learning is performed by minimizing the associated squared error function on the training set using a regularized stochastic gradient descent algorithm. the procedure is analogous to the one involving the original svd algorithm [8]; details are omitted here for brevity. time complexity per iteration is still linear with the input size, while wall clock running time is approximately doubled compared to svd, due to the extra overhead required for updating the temporal parameters. importantly, convergence rate was not affected by the temporal parameterization, and the process converges in around 30 iterations. addressing temporal dynamics leads to significant accuracy gains within the movie rating dataset, when considering past rmse improvements on the dataset. in table 2 we compare results of three algorithms. first is the plain matrix factorization algorithm as per 3, denoted by svd. second, is the svd method 4, which was considered as a significant improvement over svd by incorporating also a kind of implicit feedback [8]. finally is the newly proposed timesvd, which accounts for temporal effects as in 14. the three methods are compared over a range of factorization dimensions f . all methods benefit from a growing number of factor dimensions, what enables them to better express complex movieuser interactions. notice that the improvement delivered by timesvd over svd is consistently more significant than the improvement svd achieves over svd. in fact, we are not aware of any single algorithm in the literature that could deliver such accuracy. we attribute this to the importance of properly addressing temporal effects. what further demonstrates the importance of capturing temporal dynamics is the fact that a timesvd model of dimension 10 is already more accurate than an svd model of dimension 200. similarly, a timesvd model of dimension 20 is enough to outperform an svd model of dimension 200. 5. temporal dynamics at neighborhood models the most common approach to cf is based on neighborhood models. while typically less accurate than their factorization counterparts, neighborhood methods enjoy popularity thanks to some of their merits, such as explaining the reasoning behind computed recommendations, and seamlessly accounting for new entered ratings. recently, we suggested an itemitem model based on global optimization [8], which will enable us here to capture time dynamics in a principled manner. the static model, without temporal dynamics, is centered on the following prediction rule: ^ r you b b r you r b w c 15 here, the itemitem weights w and c represent the adjustments we need to make to the predicted rating of item i , given a known rating of item j . it was proven greatly beneficial to use two sets of itemitem weights: one the w s is related to the values of the ratings, and the other disregards the rating value, considering only which items were rated the c s. these weights are automatically learnt from the data together with the biases b and b . the constants b are precomputed according to 2. recall that r you is the set of items rated by user you . when adapting rule 15 to address temporal dynamics, two components should be considered separately. first, is the baseline predictor portion, you b b , which explains most of the observed signal. second, is the part that captures the more informative signal, dealing with useritem interaction r you r b w c . as for the baseline part, nothing changes from the factor model, and we replace it with you b t b t , according to 6 and 9. however, capturing temporal dynamics within the interaction part requires a different strategy. itemitem weights w and c reflect inherent item characteristics and are not expected to drift over time. learning process should make sure that they capture unbiased long term values, without being too affected from drifting aspects. indeed, the time changing nature of the data can mask much of the longer term itemitem relationships if not treated adequately. for instance, a user rating both items i and j high in a short time period, is a good indicator for relating them, thereby pushing higher the value of w . on the other hand, if those two ratings are given five years apart, while the user's taste if not her identity could considerably change, this is less of an evidence of any relation between the items. on top of this, we would argue that those considerations are pretty much userdependent some users are more consistent than others and allow relating their longer term actions. our goal here is to distill accurate values for the itemitem weights, despite the interfering temporal effects. first we need to parameterize the decaying relations between two items rated by user you . we adopt exponential decay formed by the function e , where b 0 controls the user specific decay rate and should be be learnt from the data. we also experimented with other decay forms, such as a power law decay t , which resulted in slightly inferior results. this leads to the prediction rule: ^ r t you b t b t 16 r you e r b w c here, in a slight abuse of notation, we assume that the set r you contains not only the items rated by you , but also the time of those ratings. the involved parameters, b t b b , b t b a dev t b , b , w and c , are learnt by minimizing the associated regularized squared error: r t you b b b a dev t b r you e r b w c l b b b a b w c 17 minimization is performed by stochastic gradient descent. we run the process for 25 iterations, with l 0 . 002 , and step size learning rate of 0.005. an exception is the update of the exponent b , where we are using a much smaller step size of 10 . training time complexity is the same as the original algorithm, which is: o r you . one can tradeoff complexity with accuracy by sparsifying the set of itemitem relations as explained in [8]. like in the factor case, properly considering temporal dynamics improves the accuracy of the neighborhood model within the movie ratings dataset. the rmse decreases from 0.9002 [8] to 0.8885. to our best knowledge, this is significantly better than previously known results by neighborhood methods. to put this in some perspective, this result is even better than those reported [1, 10, 21] by using hybrid approaches such as applying a neighborhood approach on residuals of other algorithms. a lesson is that addressing temporal dynamics in the data can have a more significant impact on accuracy than designing more complex learning algorithms. we would like to highlight an interesting point related to the basic methodology described in sec. 3. let you be a user whose preferences are quickly drifting b is large. hence, old ratings by you should not be very influential on his status at the current time t . one could be tempted to decay the weight of you 's older ratings, leading to "instance weighting" through a cost function like: e r you b b b a dev t b r you r b w c l such a function is focused at the current state of the user at time t , while deemphasizing past actions. we would argue against this choice, and opt for equally weighting the prediction error at all past ratings as in 17, thereby modeling all past user behavior. this allows us to exploit the signal at each of the past ratings, a signal that is extracted as itemitem weights. learning those weights would equally benefit from all ratings by a user. in other words, we can deduce that two items are related if users rated them similarly within a short timeframe, even if this happened long ago. 6. an exploratory study in fig. 1 we showed two strong temporal effects within the netflix movierating data. first effect exhibits a sudden rise in the average movie rating beginning around 1500 days into the dataset, corresponding to early 2004. the second effect shows that people tend to give higher ratings as movies become older movie age is measured by number of days since its first rating in the dataset. the patterns that our temporal models capture may help in explaining what created those two global temporal effects. let us start with the first effect. we can come up with several hypotheses on what caused the sudden increase of rating scores. 1. since 2004 people are matched with movies better suited for them leading to higher entered ratings. this may result by technical improvements in netflix recommendation technology cinematch and/or gui improvements making people more aware of movies they like. notice that an improvement in cinematch's effectiveness can have a significant impact on which movies members rent and subsequently rate, as cinematch suggestions drive 60 of netflix's rentals [20]. 2. since 2004 people are biased to give higher ratings in general. a possible because is a hypothetical change of the labels associated with the star scores. while at the present, stars reflect subjective feelings on the movies e.g., 5 stars"loved it", 4 stars"really liked it", in the past they might have used to denote a more objective quality e.g., 5 stars"superb movie", setting a higher bar for a 5 star rating. 3. the vast majority of the users in the netflix dataset gave their first rating no earlier than 2004. it is possible that unlike early adopters those newer users have a less refined taste and shifted the overall rating average higher. a straightforward analysis rejects the third hypothesis even when concentrating on earlier customers, e.g., those who have rated earlier than 2003, we can find a strong shift in rating scale since early 2004. as for the two other hypotheses, we use our models for examining them. the first hypothesis corresponds to the interaction part of the models e.g., q p t r you y for the timesvd model, which measures how well user and movie characteristics match together. on the other hand, the second hypothesis, deals with general biases that have nothing to do with the matching of users to movies. thus, it corresponds to the baseline predictor portion of them model you b t b t . in order to analyze this, we modeled the data using the timesvd model of dimensionality f 50 . while the full model could accurately regenerate the shifts in rating values over time, more interesting to us is to separate the model predictions into baseline and interaction components, and examine how each of them evolve over time. results are shown in fig. 2. we observe that since early 2004 1500 days into the data, the score due to interaction between users and movies steadily rises, indicating that users are increasingly rating movies that are more suitable for their own taste. this supports the first hypothesis of an ongoing improvement in the way netflix matches users to movies beginning at early 2004 and continuing since then. apparently, this could be expected knowing the large effort that company invests in improving their recommender system [4]. at the same time, the various biases, captured by the baseline predictors, exhibit a onetime phase transition around the 1500 days time point. while shallower than the change in the interaction part, the jump is clear and supports the more surprising second hypothesis. this hints that beyond a constant improvement in matching people to movies they like, something else happened in early 2004 causing an overall shift in rating scale. uncovering this may require extra information on the related circumstances. figure 2: tracking the change of the two components of modeled movieratings over time. first component "baseline" represents rating behavior influenced by exterior considera tions, while the other component "interaction" captures the rating behavior that is explained by the match between users and movies. now we move to the second temporal effect. we would like to suggest two hypotheses to why ratings rise as movies become older. 1. older movies are getting rated by users better matching them. this might indicate that people watch and then rate a new movie even if it is less appropriate for them, but will watch an older movie only after a more careful selection process. such an improved match between users and movies can be captured by the fact that the interaction part of the model is rising with movies' age. 2. older movies are just inherently better than newer ones. this would be captured by the baseline part of the model. once again we split the modeled behavior into two parts the interaction part measuring the match between users and movies and the baseline part capturing other effects. in fig. 3 we track how those two components change over time. much like the original raw data in fig. 1, the interaction part shows a steady increase virtually throughout the whole movie age range at a close to linear pace. on the other hand, the baseline portion is increasing only between days 1000 and 1500, while being captured within a narrow range elsewhere. since we measure movie age by number of days since first rating, as movies become older they are more exposed to the aforementioned early 2004 rating jump effect. in particular, all movies older than 1500 days must be fully susceptible to this effect. thus, it is possible that the increase in baseline values between days 1000 and 1500 reflects such a side effect. to wipe out this interfering effect we concentrate only on ratings to movies aged 1500 days or older. this leaves us with about 44 of the points 44 million rating instances and makes the picture much clearer. while the raw ratings as well the interaction part continue to steadily increase beyond day 1500, the baseline portion does not increase after that day. we view this as an indication that the first hypothesis is closer to the truth than the second one. figure 3: tracking the change of the two components of modeled ratings against age of rated movie. we observe a consistent improvement in the match between users and movies as movie age rises captured by the "interaction" component. 7. related works in the past few years, much research was devoted to the netflix dataset. many works were published in the two kdd workshops dedicated to that dataset [3, 23]. other notable works include [8, 13, 19]. best reported results were obtained by integrating the factorization and neighborhood models. results reported in this paper by pure factorization are more accurate, in a sense showing that addressing temporal dynamics is not less important than algorithmic sophistication created by integration of two different models. despite the high impact of temporal effects on user preferences, the subject attracted a quite negligible attention in the recommender literature. notable discussions of temporal effects include ding and li [6], who suggested a time weighting scheme for a similaritybased collaborative filtering approach. at the prediction stage, similarities to previously rated items are decayed as time difference increases. the decay rate is both userdependent and itemdependent. sugiyama et al. [17] proposed a personalized web search engine, where they let the user profile evolve over time. there, they distinguish between aspects of user behavior computed over a fixed time decay window, and ephemeral aspects captured within the current day. in a prior work, we suggested an incremental modeling of global effects [1], which include some baseline time effects. this scheme was later enhanced [11, 21]. our work is within the topic of tracking and modeling concept drift, which has gathered much interest in the data mining community. early works in the field e.g. [15, 25] used techniques like adjusted and decayed weights of past instances or using a sliding time window. another approach popular in newer publications e.g. [7, 16, 24] is based on maintaining an ensemble of models capable of capturing various states of the data. as explained in sec. 3, the problem of tracking user preferences, especially in a collaborative filtering scenario, requires different approaches. 8. conclusions tracking the temporal dynamics of customer preferences to products raises unique challenges. each user and product potentially goes through a distinct series of changes in their characteristics. moreover, we often need to model all those changes within a single model thereby interconnecting users or, products to each other to identify communal patterns of behavior. a mere decay of older instances or usage of multiple separate models lose too much signal, thus degrading prediction accuracy. the solution we adopted is to model the temporal dynamics along the whole time period, allowing us to intelligently separate transient factors from lasting ones. we applied this methodology with two leading recommender techniques. in a factorization model, we modeled the way user and product characteristics change over time, in order to distill longer term trends from noisy patterns. in an itemitem neighborhood model, we showed how the more fundamental relations among items can be revealed by learning how influence between two items rated by a user decays over time. in both factorization and neighborhood models, the inclusion of temporal dynamics proved very useful in improving quality of predictions, more than various algorithmic enhancements. this led to the best results published so far on a widely analyzed movie rating dataset. 9. references [1] r. bell and y. koren. scalable collaborative filtering with jointly derived neighborhood interpolation weights. ieee international conference on data mining icdm'07 , pp. 4352, 2007. [2] r. m. bell, y. koren and c. volinsky. modeling relationships at multiple scales to improve accuracy of large recommender systems. proc. 13th acm sigkdd international conference on knowledge discovery and data mining kdd'07 , pp. 95104, 2007. [3] j. bennett, c. elkan, b. liu, p. smyth and d. tikk eds.. kdd cup and workshop in conjunction with kdd'07 , 2007. [4] j. bennet and s. lanning. the netflix prize. kdd cup and workshop , 2007. www.netflixprize.com [5] j. canny. collaborative filtering with privacy via factor analysis. proc. 25th acm sigir conf. on research and development in information retrieval sigir'02 , pp. 238245, 2002. [6] y. ding and x. li. time weight collaborative filtering. proc. 14th acm international conference on information and knowledge management cikm'04 , pp. 485492, 2004. [7] j. z. kolter and m. a. maloof. dynamic weighted majority: a new ensemble method for tracking concept drift. proc. ieee conf. on data mining icdm'03 , pp. 123130, 2003. [8] y. koren. factorization meets the neighborhood: a multifaceted collaborative filtering model. proc. 14th acm sigkdd int. conf. on knowledge discovery and data mining kdd'08 , pp. 426434, 2008. [9] g. linden, b. smith and j. york. amazon.com recommendations: itemtoitem collaborative filtering. ieee internet computing , 7:7680, 2003. [10] a. paterek. improving regularized singular value decomposition for collaborative filtering. proc. kdd cup and workshop , 2007. [11] g. potter. putting the collaborator back into collaborative filtering. kdd'08 workshop on large scale recommenders systems and the netflix prize , 2008. [12] p. pu, d. g. bridge, b. mobasher and f. ricci eds.. proc. 2008 acm conference on recommender systems , 2008. [13] r. salakhutdinov, a. mnih and g. hinton. restricted boltzmann machines for collaborative filtering. proc. 24th annual international conference on machine learning , pp. 791798, 2007. [14] b. sarwar, g. karypis, j. konstan and j. riedl. itembased collaborative filtering recommendation algorithms. proc. 10th international conference on the world wide web , pp. 285295, 2001. [15] j. schlimmer and r. granger. beyond incremental processing: tracking concept drift. proc. 5th national conference on artificial intelligence , pp. 502507, 1986. [16] w. n. street and y. kim. a streaming ensemble algorithm sea for largescale classification. proc. 7th acm sigkdd international conference on knowledge discovery and data mining kdd'01 , pp. 377382, 2001. [17] k. sugiyama, k. hatano and m. yoshikawa. adaptive web search based on user profile constructed without any effort from users. proc. 13th international conference on world wide web www'04 , pp. 675684, 2004. [18] g. takacs, i. pilaszy, b. nemeth and d. tikk. major components of the gravity recommendation aystem. sigkdd explorations , 9:8084, 2007. [19] g. takacs, i. pilaszy, b. nemeth and d. tikk. matrix factorization and neighbor based algorithms for the netflix prize problem. proc. 2008 acm conference on recommender systems recsys'08 , pp. 267274, 2008. [20] c. thompson. if you liked this, you are sure to love that. the new york times , nov 21, 2008. [21] a. toscher, m. jahrer and r. legenstein. improved neighborhoodbased algorithms for largescale recommender systems. kdd'08 workshop on large scale recommenders systems and the netflix prize , 2008. [22] a. tsymbal. the problem of concept drift: definitions and related work. technical report tcdcs200415, trinity college dublin, 2004. [23] a. tuzhilin, y. koren, j. bennett, c. elkan and d. lemire eds.. workshop on large scale recommender systems and the netflix prize in conjunction with kdd'08 , 2008. [24] h. wang, w. fan, p. s. yu, and j. han. mining concept drifting data streams using ensemble classifiers. proc. 9th acm sigkdd international conference on knowledge discovery and data mining kdd'03 , pp. 226235, 2003. [25] g. widmer and m. kubat. learning in the presence of concept drift and hidden contexts. machine learning , 23:69101, 1996.